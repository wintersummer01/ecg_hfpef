{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8944281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from config import BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50965972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful variables & data loading\n",
    "\n",
    "raw_data_root = '/home/ubuntu/ecg_mount'\n",
    "proc_data_root = '/home/ubuntu/ecg_hfpef/data/processed_data'\n",
    "file_root = './fname_score_pair.csv'\n",
    "crit_file_root = './fname_crit_pair.csv'\n",
    "keys = ['als', 'fname', 'score']\n",
    "crit_keys = ['als', 'fname', 'BMI', 'hypertensive', 'afib', 'pulm hyper', 'age', 'pressure']\n",
    "\n",
    "score_data = pd.read_csv(f'{raw_data_root}/H2FPEF-echo_.csv', encoding='cp949', low_memory=False)\n",
    "fname_data = pd.read_csv(f'{raw_data_root}/230111_SevMUSE_EKG_MasterTable.csv', low_memory=False)\n",
    "\n",
    "num_datas = len(score_data)\n",
    "batch_size = 20000\n",
    "num_batches = num_datas//batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d151b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset CSV File. \n",
    "# Please be careful to run the cell. \n",
    "\n",
    "if os.path.isfile(file_root):\n",
    "    os.remove(file_root)\n",
    "\n",
    "empty = pd.DataFrame(columns = keys)\n",
    "empty.to_csv(file_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c815b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that processes single batch\n",
    "# From mini_score_df, get fname from fname_df, then append into main_df\n",
    "\n",
    "def dataBatchProcessing(main_df, mini_score_df, fname_df):\n",
    "    for i in tqdm(range(len(mini_score_df))):\n",
    "        als = mini_score_df.iloc[i]['AlsUnitNo']\n",
    "        ecg_fnames = fname_df.query(f'AlsUnitNo=={als}')\n",
    "        \n",
    "        # Skip if there is no ecg data, or als is already in main_df\n",
    "        if len(ecg_fnames) == 0 or als in main_df['als'].tolist():\n",
    "            continue\n",
    "        \n",
    "        # Find ecg filename that has minimum date diffrence. \n",
    "        fname_index = None\n",
    "        echo_date = datetime.datetime.strptime(mini_score_df.iloc[i]['STUDYDATE'], '%Y-%m-%d')\n",
    "        min_date_diff = 9999999\n",
    "        for j in range(len(ecg_fnames)):\n",
    "            # Check if the waveform has shape (5000, 12)\n",
    "            if ecg_fnames.iloc[j]['waveform_shape'] != '(5000, 12)':\n",
    "                continue\n",
    "            # Minimum data difference\n",
    "            ecg_date = datetime.datetime.strptime(ecg_fnames.iloc[j]['AcqDate'], '%Y-%m-%d')\n",
    "            date_diff = abs((echo_date - ecg_date).days)\n",
    "            if date_diff < min_date_diff:\n",
    "                min_date_diff = date_diff\n",
    "                fname_index = j\n",
    "        # Skip if there is no ecg data that matches the waveform shape (5000, 12)\n",
    "        if fname_index == None:\n",
    "            continue\n",
    "                \n",
    "        # Save and append to the main stream\n",
    "        fname = ecg_fnames.iloc[fname_index]['fname']\n",
    "        score = mini_score_df.iloc[i]['score_H2FPEF']\n",
    "        temp = pd.DataFrame({'als':als, 'fname':fname, 'score':score}, index=[0])\n",
    "        main_df = pd.concat([main_df, temp], ignore_index=True)\n",
    "    \n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f749e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing function.\n",
    "# divides data into batches and runs dataBatchProcessing for each batch\n",
    "\n",
    "def main(score, fname, num_data, bs, end_batch, file_root, start_batch=0):\n",
    "    for i in range(start_batch, end_batch):\n",
    "        # Get size of the batch for the minibatch index\n",
    "        if i == end_batch:\n",
    "            size = num_data%bs\n",
    "        else:\n",
    "            size = bs\n",
    "        \n",
    "        # Read mainstream from csv\n",
    "        data = pd.read_csv(file_root)\n",
    "        mini_batch = score.iloc[i*bs:i*bs+size]\n",
    "        \n",
    "        # Process and save to csv\n",
    "        print(f'Processing batch {i}/{end_batch}')\n",
    "        data = dataBatchProcessing(data, mini_batch, fname)\n",
    "        data.to_csv(file_root)\n",
    "        print(f'Done batch {i} processing\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c72d3a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [08:43<00:00, 38.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 0 processing\n",
      "\n",
      "Processing batch 1/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [07:12<00:00, 46.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 1 processing\n",
      "\n",
      "Processing batch 2/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [07:23<00:00, 45.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 2 processing\n",
      "\n",
      "Processing batch 3/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [07:44<00:00, 43.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 3 processing\n",
      "\n",
      "Processing batch 4/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [07:59<00:00, 41.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 4 processing\n",
      "\n",
      "Processing batch 5/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [08:24<00:00, 39.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 5 processing\n",
      "\n",
      "Processing batch 6/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [08:42<00:00, 38.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 6 processing\n",
      "\n",
      "Processing batch 7/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [08:43<00:00, 38.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 7 processing\n",
      "\n",
      "Processing batch 8/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [08:48<00:00, 37.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 8 processing\n",
      "\n",
      "Processing batch 9/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [08:55<00:00, 37.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 9 processing\n",
      "\n",
      "Processing batch 10/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [09:38<00:00, 34.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 10 processing\n",
      "\n",
      "Processing batch 11/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [10:02<00:00, 33.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 11 processing\n",
      "\n",
      "Processing batch 12/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [10:19<00:00, 32.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 12 processing\n",
      "\n",
      "Processing batch 13/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [10:50<00:00, 30.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 13 processing\n",
      "\n",
      "Processing batch 14/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [11:03<00:00, 30.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 14 processing\n",
      "\n",
      "Processing batch 15/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 20000/20000 [10:54<00:00, 30.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 15 processing\n",
      "\n",
      "Processing batch 16/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 670/670 [00:21<00:00, 31.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done batch 16 processing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run main\n",
    "\n",
    "main(score_data, fname_data, num_datas, \\\n",
    "     batch_size, num_batches, file_root, start_batch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5411d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Addtional terms from batch calculation\n",
    "\n",
    "drop_columns = [\"Unnamed: 0\"] + [f\"Unnamed: 0.{i}\"for i in range(1, num_batches+1)]\n",
    "data = pd.read_csv(file_root)\n",
    "new_data = data.drop(columns=drop_columns)\n",
    "new_data.to_csv(file_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d6fe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7\n"
     ]
    }
   ],
   "source": [
    "# score min max checking\n",
    "\n",
    "data = pd.read_csv(file_root)\n",
    "print(min(data['score']), max(data['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset crit_CSV File. \n",
    "# Please be careful to run the cell. \n",
    "\n",
    "crit_file_root = './fname_crit_pair.csv'\n",
    "crit_keys = ['idx', 'als', 'fname', 'score' \\\n",
    "    'BMI', 'h-tense', 'afib', 'pulm h-tense', 'age', 'pressure']\n",
    "\n",
    "if os.path.isfile(crit_file_root):\n",
    "    os.remove(crit_file_root)\n",
    "\n",
    "empty = pd.DataFrame(columns = crit_keys)\n",
    "empty.to_csv(crit_file_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6258bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprocessing Data\n",
    "# 1. Slice data by batchsize, and re-save as numpy file\n",
    "# 2. Save Hfpef scores with each standards\n",
    "# Heavy:2, Hypertensive:1, Artrial Fibrillation:3, \n",
    "# Pulmonary Hypertnesion:1, Elder:1, Filling Pressure:1\n",
    "\n",
    "def processData(idx):\n",
    "    df_line = data_pairs.iloc[idx]\n",
    "    als, fname, score = df_line['als'], df_line['fname'], df_line['score']\n",
    "    \n",
    "    new_line = pd.DataFrame(columns = crit_keys)\n",
    "    score_lines = score_data.query(f\"AlsUnitNo=={als}\")\n",
    "    for j in len(score_lines):\n",
    "        line = score_lines.iloc[j]\n",
    "        if line['score'] == df_line['score']:\n",
    "            # From previous file\n",
    "            new_line['idx']   = idx\n",
    "            new_line['als']   = als\n",
    "            new_line['fname'] = fname\n",
    "            new_line['score'] = score\n",
    "            \n",
    "            # Get each criteria (new)\n",
    "            new_line['BMI']          = line['머시기']\n",
    "            new_line['h-tense']      = \n",
    "            new_line['afib']         = \n",
    "            new_line['pulm h-tense'] = \n",
    "            new_line['age']          = \n",
    "            new_line['pressure']     = \n",
    "            break\n",
    "        \n",
    "    ecg_data = pd.read_csv(fname).to_numpy()\n",
    "    return ecg_data, new_line\n",
    "\n",
    "\n",
    "data_pairs = pd.read_csv(file_root)\n",
    "batch_size = BATCH_SIZE\n",
    "num_batches = len(data_pairs)//batch_size\n",
    "batch_idx = [i*batch_size for i in range(num_batches)] + [len(data_pairs)]\n",
    "num_threads = 16\n",
    "\n",
    "for i in range(num_batches+1):\n",
    "    crit_df = pd.read_csv\n",
    "    \n",
    "    batch_pairs = data_pairs.iloc[batch_idx[i]:batch_idx[i+1]]\n",
    "    idx = range(len(batch_pairs))\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        batch_results = list(tqdm(executor.map(processData, idx), total=len(batch_pairs)))\n",
    "        \n",
    "    ecg_data = []\n",
    "    for ecg_datum, new_line in batch_results:\n",
    "        ecg_data.append(ecg_datum)\n",
    "        main_df = pd.concat([main_df, temp], ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
